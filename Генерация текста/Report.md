# Отчет по лабораторной работе "Генерация последовательностей"

### Бабичева Анна, М8О-304Б-17
Номер в группе: 2, Вариант: 2 ((остаток от деления (2-1) на 6)+1)

> Оценка: 3.7. *Опоздание в выполнении работы. Использованы nltk и Word2Vec (это хорошо!), но в отчёте толком не описано, что это такое и зачем.*

### Цель работы

В данной работе предстоит научиться генерировать последовательности при помощи рекуррентных нейронных сетей. В моем варианте качестве последовательности выступает проза на русском языке, элемент последовательности - одно слово. 

Необходимо исследовать следующие нейросетевые структуры: 

- Обычная полносвязная RNN
- Однослойная LSTM
- Двухслойная LSTM
- Однослойный GRU
   
### Используемые входные данные

В качестве данных для обучения я использовала текст произведения М. Булгакова "Записки юного врача". Текст был взят на http://lib.ru.

### Предварительная обработка входных данных

Текст после преобразования с помощью нужной кодировки имел ряд особенностей. Во-первых, в конце текста присутствовала техническая информация, которая не имеет смысла при обучении. Во-вторых, каждая новая глава, а также название самой книги, были "окружены" тегами html. В-третьих, диалоги в книге обозначались "-", поэтому текст был перенасыщен дефисами. В-четвертых, часто в книге попадалась конструкция "xxx", что не несло никакой информации для дальнейшего обучения. Поэтому необходимо было "почистить" текст для его дальнейшей обработки.

1. Замечаем, что в самом начале содержится большая конструкция с тегами html, которую можно обрезать до тега "\<pre>".

2. Определяем ряд символов, которые просто удалим из текста.

3. Каждая новая глава начинается с одинаковой конструкции из тегов html, поэтому ее обрезка будет единообразной. При этом мы потеряем названия глав, но считаю эту потерю несущественной при обучении. Так же было принято решение сохранять отдельные главые, как единое целое, чтобы при дальнейшем составлении последовательностей не терять или не преобретать дополнительный смысл путем "заимствований" слов из другой главы.

4. После всех преобразований получаем список с главами книги. Однако некоторые главы были вырезаны с некоторыми погрешностями, что привело к появлению пустых глав. Удаляем эти пустые главы.

``` 
def clear_html(text):
  res = text
  res = res.replace('<pre>', '', 1)
  res = res[res.find('<pre>'):]
  res = res.replace('<pre>', '', 1)
  res = res[:res.find('<pre>')]

  to_del = ['\n', '\t', 'x x x', '-']
  for d in to_del:
    res = res.replace(d, ' ')

  t = res
  res = []

  while '</ul>' in t:
    t = t.replace('<ul>', '', 1)
    t_ = t[t.find('</ul>'):t.find('<ul>')]
    res.append(t_.replace('</ul>', '', 1)) 
    t = t[t.find('<ul>'):]  

  for s in res:
    if (s == '') | (s == '   ') | (s == '    '):
      res.remove(s)
  return res
```

#### Замечание:
В самый последний момент книга по какой-то причине перестала считываться корректно, поэтому было принято решение обучаться на книге Б. Аунина "Азазель". В функцию по "очистке" была добавлена строка для удаления технической информации:

```
res = res[res.find('Евгений Морозов'):]
```

Далее нам необходимо поделить текст на последовательности определенной длины. Для этого следует токенизировать текст. При токенизации буду учитывать имена собственные, а остальные слова приведу к нижнему регистру. Также "устраню" разбивку по главам, объединив все в один список.

```
def make_tokens(text):
  res = []
  tags = nltk.pos_tag(nltk.word_tokenize(text))
  for tag in tags:
    if tag[1] != 'NNP':
      res.append(tag[0].lower())
    else:
      res.append(tag[0])
  return res
  
def make_seqs(text, lenght=50):
  ch_seqs = []
  seqs = []
  for chap in text:
    tokens = make_tokens(chap)
    i = 0
    for j in range(lenght, len(tokens), lenght):
      ch_seqs.append(tokens[i:j])
      i = j
    seqs.extend(ch_seqs)
  return seqs
```
Далее преобразую полученный список к типу Word2Vec.

Модель будет обучаться на числовом датасете, поэтому необходимо преобразовать полученное множество слов в числа. Для этого сделаю словарь с ключом - словом, а значением - его номером. А для дальнейшей  интерпретации сгенерированного текста необходимо обратное преобразование, которое будет осуществляться с помощью "инвертированного" словаря.

```
def make_vocabs(w2v):
  ind = 0
  w_i = {}
  i_w = {}
  for w in w2v.wv.vocab:
    w_i[w] = ind
    i_w[i] = w
    ind += 1
  return w_i, i_w
```

Теперь нам необходимо перевести текст на числовой язык. Для этого каждое слово каждой последовательности будем переводить с помощью ранее созданного словаря в числа. При создании Word2Vec использовались только слова, появившиеся в тексте хотя бы 2 раза, поэтому есть вероятность попасть на слово-одиночку. Чтобы не возникло ошибки при доступе по ключу, будем обращаться к другому слову. Так как в русском языке особенно много пунктуационных знаков, то будем интерпретировать такие слова, как запятые.

```
def translate_word(w, w_i):
  if w in w_i:
    return w_i[w]
  return w_i[',']
  
def translate_text(seqs, w_i):
  res = []
  for seq in tqdm(seqs):
    res.append(list(map(lambda x: translate_word(x, w_i), seq)))
  return res
```

Далее датасет был разделен на обучающую и тестовую выборки, были созданы батчи.

### Эксперимент 1: RNN

#### Архитектура сети

```
RNN
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (128, None, 510)          8457840   
_________________________________________________________________
simple_rnn (SimpleRNN)       (128, None, 256)          196352    
_________________________________________________________________
dense (Dense)                (128, None, 16584)        4262088   
=================================================================
Total params: 12,916,280
Trainable params: 12,916,280
Non-trainable params: 0
_________________________________________________________________

```

#### Результат

Последовательность:

'Сегодня хорошая погода, спортсмены и удалилась в сторону , но не трус , но как ? спросил , однако позвольте узнать за нас не видел , скорее риторический , самый раз было'

![График](%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D0%BA%D0%B8/rnn.png)

#### Вывод по данному эксперименту

Под конец обучения модель стала слегка переобучаться.

### Эксперимент 2: однослойная LSTM

#### Архитектура сети

```
1 LAYER LSTM
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (128, None, 510)          8457840   
_________________________________________________________________
lstm (LSTM)                  (128, None, 256)          785408    
_________________________________________________________________
dense_1 (Dense)              (128, None, 16584)        4262088   
=================================================================
Total params: 13,505,336
Trainable params: 13,505,336
Non-trainable params: 0
_________________________________________________________________

```

#### Результат

Последовательность:

'Сегодня хорошая погода, но проклятая какой уж объяснить . мокну . ну что примерещилось . кажется , чтобы в оперетке , в нем так ! прошипела храбрая свидетельство маршрутную  азазель ,'

![График](%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D0%BA%D0%B8/lstm_1.png)

#### Вывод по данному эксперименту

Под конец обучения модель стала слегка переобучаться.

### Эксперимент 3: двухслойная LSTM

#### Архитектура сети

```
2 LAYERS LSTM
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_2 (Embedding)      (128, None, 510)          8457840   
_________________________________________________________________
lstm_1 (LSTM)                (128, None, 256)          785408    
_________________________________________________________________
lstm_2 (LSTM)                (128, None, 256)          525312    
_________________________________________________________________
dense_2 (Dense)              (128, None, 16584)        4262088   
=================================================================
Total params: 14,030,648
Trainable params: 14,030,648
Non-trainable params: 0
_________________________________________________________________

```

#### Результат

Последовательность:

'Сегодня хорошая погода. я тебе адрес крови невдомек . это лицо мужа управления ласковый . банкомет двери жестоко отпили человек . но ведь обошлось , а кроме втором два счета : '

![График](%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D0%BA%D0%B8/lstm_2.png)

#### Вывод по данному эксперименту

Под конец обучения модель стала слегка переобучаться.

### Эксперимент 4: GRU

#### Архитектура сети

```
GRU
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_3 (Embedding)      (128, None, 510)          8457840   
_________________________________________________________________
gru (GRU)                    (128, None, 256)          589824    
_________________________________________________________________
dense_3 (Dense)              (128, None, 16584)        4262088   
=================================================================
Total params: 13,309,752
Trainable params: 13,309,752
Non-trainable params: 0
____________________________
_________________________________________________________________

```

#### Результат

Последовательность:

'Сегодня хорошая погода, Амалия бога понял добрый . тут то додумался счет . раскрыв друг мой ориентацию как вытащиться , еще садитесь дорогой перчаток перчаток вас одни границей что отдал знакомство затаился'

![График](%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D0%BA%D0%B8/gru.png)

#### Вывод по данному эксперименту

Под конец обучения модель стала слегка переобучаться.

### Выводы

Данная лабораторная работа позволила построить 4 вида рекуррентных нейронных сетей и понаблюдать, как каждая из них сгенерирует последовательности.

В результате были получены последовательности на русском языке, которые носитель данного языка скорее всего сочтет бессмыслицей, однако хочется отметить, что модели в большинстве случаев сумели уловить грамматические особенности языка. По моим предположениям, последовательности такого качества получились по нескольким причинам:

1. Русский язык достаточно сложен, и на его качественную обработку необходимо тратить больше времени. Можно было провести более качественную "очистку" книги, дробить текст на последовательности разной длины, а не фиксированной, в зависимости от контекста, вручную отделять имена собственные и т.д.

2. Архитектура самих моделей была очень проста, оптимальные параметры не были подобраны ввиду ограничеснности времени и медлительности ЭВМ. Можно было провести подбор параметров моделей с помощью GridSearch и более осмысленно подойти к построению архитектуры моделей.

3. Судя по графикам потерь, модели под конец стали получать "скачащий" результат по кросс-энтропии. Это может говорить об их переобучении. Можно было подобрать оптимальное количество эпох для каждой модели.

4. Сама книга могла быть недостаточно большого размера для обучения. Можно было обучаться на ряде книг, которые были заготовлены вначале, но это не было сделано ввиду ограниченности времени.

5. Отслеживание процесса обучения было проделано без кросс-валидации, что не позволило своевременно оценивать качество моделей.

Но несмотря на то, что мои модели были далеко не совершенны, я считаю, что получила неплохую практику в проектировании рекуррентных сетей, обработке естественного языка и работе с сервисом Google Collab. К тому же, теперь я стала на шаг ближе к разгадке того, как боты ведут с нами, людьми, "диалог".
